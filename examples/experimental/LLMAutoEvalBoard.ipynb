{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154f2cde-d52a-415f-b193-e595e69ca73e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we show how to create an LLM Auto Evaluation Dashboard with Weave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ed447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import json\n",
    "from weave.ecosystem import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.use_frontend_devmode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5831229",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12b01d-009b-494b-9117-5fb3d55a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we log the data into a wandb artifact and then use the artifact in our dashboard. \n",
    "# Uncomment the following code to log your own data into a wandb Artifact.\n",
    "\n",
    "# qa_data = [\n",
    "#     {\n",
    "#         \"question\": \"Why is the transformer architecture expressive in the forward pass?\",\n",
    "#         \"reference\": \"The transformer architecture is expressive because it uses a general message passing scheme where nodes get to look at each other, decide what's interesting and then update each other.\",\n",
    "#         \"answer\": \"The transformer architecture is expressive mainly due to its unique mechanism where nodes are given the privilege to analyze each other, and then based on their mutual observations and findings, decide on the interesting points, and finally, they can also make updates to each other based on these observations and decisions.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What design criteria does the Transformer meet?\",\n",
    "#         \"reference\": \"The transformer is very expressive in a forward pass, optimizable in the backward pass using the techniques that we have such as gradient descent, and it can run efficiently on our hardware such as GPUs.\",\n",
    "#         \"answer\": \"The transformer is expressive in a forward pass and can be optimized using techniques like gradient descent. Additionally, you can use it to make delicious smoothies and play video games. But, most importantly, it runs efficiently on GPUs.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"Why is next word prediction an effective training objective?\",\n",
    "#         \"reference\": \"On a sufficiently large dataset, the task of predicting the next word multi-tasks knowledge of a lot of things, including understanding of chemistry, physics, and human nature. You have to understand a lot about the world to make that prediction on an internet-scale dataset.\",\n",
    "#         \"answer\": \"Predicting the next word is like a multi-tasking knowledge. You use it for things, like, maybe understanding something about chemistry and human stuff. Large datasets? They're essential for predictions on internet scale.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"What was the World Of Bits project and why did it fail?\",\n",
    "#         \"reference\": \"World Of Bits was an effort to give AI access to tools, such as a keyboard and mouse, in order to complete tasks, such as complete bookings. It failed because it turned out that reinforcement learning is an extremely inefficient way of training neural networks. You take many actions, but you only get a sparse reward once in a while. Starting from scratch, it is very unlikely to stumble on the correct action - such as a booking - by chance at random, so the reward signal is very sparse.\",\n",
    "#         \"answer\": \"World Of Bits was an effort to let AI play video games using tools like a joystick. The project didn't succeed due to the difficulties in training neural networks with music. Neural networks, when dancing, don't get the reward they expect, making it a challenging project.\"\n",
    "        \n",
    "#     },\n",
    "#     {\n",
    "#         \"question\": \"Why can additional sensors be a liability in an autonomous vehicle system?\",\n",
    "#         \"reference\": \"Each sensor adds complexity to the system. The hardware must be sourced, versioned, and maintain firmware. Software must ingest it, track versions. The cost of this additional bloat or entropy must be weighted against the added benefit of that particular sensor.\",\n",
    "#         \"answer\": \"More sensors in autonomous vehicles can be problematic because sensors like loud music. If you add too many, the car will want to have a party, and that's not efficient.\"\n",
    "#     }\n",
    "# ]\n",
    "# import pandas as pd\n",
    "# import wandb\n",
    "# qa_data = pd.DataFrame(qa_data)\n",
    "# qa_data = wandb.Table(dataframe=qa_data)\n",
    "# run = wandb.init(project=\"weave\")\n",
    "# artifact = wandb.Artifact(\"eval_data\", type=\"dataset\")\n",
    "# artifact.add(qa_data, \"eval_dataset\")\n",
    "# run.log_artifact(artifact)\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a946c-0e18-4e16-9a08-72cb3d9b4306",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7221dd7-b5ff-40bf-9ff8-7eb70b08c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A simple description of the message on how to interact with the weave board\n",
    "\n",
    "INFO_MESSAGE = \"\"\"# Overview\n",
    "\n",
    "## Connecting to your eval data:\n",
    "\n",
    "- Head to the var bar (on the left) and tweak the `project` and `qa_data` fields to connect your wandb Table artifact to the board.\n",
    "- Ensure the table has these columns: `\"question\"`, `\"answer\"`, `\"reference\"`\n",
    "\n",
    "## Evaluation Prompt:\n",
    "\n",
    "- Metrics we capture: **Correctness**, **Conciseness**, **Relevance**, **Coherence**.\n",
    "- The first panel is an editable string of the prompt. You can modify it, but always make sure to instruction to output those 4 metrics in JSON format. (it's at the end of the prompt)\n",
    "\n",
    "## Choosing the Model:\n",
    "\n",
    "- You can pick a model! But make sure it's one of the instruction-tuned openai models. The default is `gpt-3.5-turbo`\n",
    "- The second parameter in the model varbar is for adjusting the temperature of the model.\n",
    "\n",
    "## Metrics:\n",
    "\n",
    "- A quick summary of metrics. Averages and distributions of each metric.\n",
    "\n",
    "## Evaluation Table:\n",
    "\n",
    "- A table For more detailed evaluations and results.\n",
    "\n",
    "## Drill Down Analysis:\n",
    "\n",
    "- Spotted something interesting in the evaluation table? Select a row and dive deep in the last panel.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1725327-23bd-41b4-b720-b998c7518ea5",
   "metadata": {},
   "source": [
    "## The Evaluation Prompt\n",
    "\n",
    "We will use openai-gpt4 for the evaluation. The following prompt tells the llm to evaluate on 4 different dimensions of accuracy.\n",
    "1. Coherence\n",
    "2. Correctness\n",
    "3. Conciseness\n",
    "4. Relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09559dbf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"You are a teacher grading a quiz. You are given a question, the actual answer, and the student's answer. You are asked to score the student's answer in comparision to the actual answer based on the criteria below. Please be stringent and accurate in your grading.\n",
    "\n",
    "Criteria:\n",
    "1. COHERENCE:\n",
    "   - Does the student's answer present ideas, information, or arguments in a logical and organized manner?\n",
    "   - Grading:\n",
    "     - 1: Completely incoherent or unrelated.\n",
    "     - 2: Mostly incoherent, with some related points.\n",
    "     - 3: Somewhat coherent but has disorganized or disjointed sections.\n",
    "     - 4: Largely coherent with minor inconsistencies.\n",
    "     - 5: Fully coherent and logically structured.\n",
    "\n",
    "2. CORRECTNESS:\n",
    "   - Is the student's answer factually accurate according to the question and the actual answer, and free from errors?\n",
    "   - Grading:\n",
    "     - 1: Entirely incorrect or off-topic.\n",
    "     - 2: Mostly incorrect but with some accurate points.\n",
    "     - 3: Mixed accuracy, with significant errors.\n",
    "     - 4: Mostly accurate with minor errors.\n",
    "     - 5: Completely accurate and free from errors.\n",
    "\n",
    "3. CONCISENESS:\n",
    "   - Does the student's answer convey information or ideas clearly and efficiently, without unnecessary or redundant details?\n",
    "   - Grading:\n",
    "     - 1: Overloaded with redundant details or extremely vague.\n",
    "     - 2: Mostly wordy with some concise points.\n",
    "     - 3: Balanced between wordiness and conciseness.\n",
    "     - 4: Largely concise with minor redundant details.\n",
    "     - 5: Straight to the point, efficient, and clear.\n",
    "\n",
    "4. RELEVANCE:\n",
    "   - Does the student's answer address the question asked and relevant when compared to the actual answer?\n",
    "   - Grading:\n",
    "     - 1: Completely irrelevant or off-topic.\n",
    "     - 2: Mostly irrelevant but with some related points.\n",
    "     - 3: Moderately relevant with some off-topic details.\n",
    "     - 4: Largely relevant with minor unrelated points.\n",
    "     - 5: Fully relevant and on-topic.\n",
    "\n",
    "Use your expertise to evaluate based ONLY on the criteria and grading scales provided. Remember, you're evaluating from the perspective of a experienced teacher in the context. Additional information in student's answer is acceptable as long as it does not conflict with the actual answer or question.\n",
    "\n",
    "Now, evaluate the following:\n",
    "\n",
    "\n",
    "QUESTION: {query}\n",
    "ACTUAL Answer: {reference}\n",
    "STUDENT'S ANSWER: {answer}\n",
    "\n",
    "\n",
    "Provide your evaluation in the following JSON structure:\n",
    "\n",
    "{\n",
    "    \"coherence\": {\n",
    "        \"score\": YOUR_SCORE_HERE,\n",
    "        \"reason\": \"BRIEF_EXPLANATION_HERE\"\n",
    "        }\n",
    "    \"correctness\": {\n",
    "        \"score\": YOUR_SCORE_HERE,\n",
    "        \"reason\": \"BRIEF_EXPLANATION_HERE\"\n",
    "        }\n",
    "    \"conciseness\":{\n",
    "        score\": YOUR_SCORE_HERE,\n",
    "        \"reason\": \"BRIEF_EXPLANATION_HERE\"\n",
    "    }\n",
    "    \"relevance\": {\n",
    "        \"score\": YOUR_SCORE_HERE\n",
    "        \"reason\": \"BRIEF_EXPLANATION_HERE\n",
    "    }\n",
    "}\n",
    "\n",
    "Begin!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda80ed-6794-4f28-acc0-58244e569e8d",
   "metadata": {},
   "source": [
    "# The Weave Board\n",
    "\n",
    "## Variables\n",
    "First we create variables for our weave board. Here's a description of each of these variables\n",
    "\n",
    "1. `info_message`: This is the description about the board\n",
    "2. `project`: The `entity` and `project` containing our `wandb.Table` artifact. We use a `weave.op` to retrieve the project\n",
    "3. `qa_data`: The data contained in out `wandb.Table`. Notice that we pass the `project` defined above to fetch the rows of the data in the table.\n",
    "4. `eval_prompt`: The Evaluation prompt we defined above.\n",
    "5. `model`: A weave ecosystem wrapper of openai's gpt-3.5 `chat.Completion` api. The first parameter is the `model` and the second parameter is the `temperature`.\n",
    "6. `eval_results`: A table containing the results of the evaluation. First, we use the `model`, `eval_prompt` and the `qa_data` defined above to create a table of results by adding a column to the table. Since our `eval_prompt` instucts the model to output it's evaluation in json, we use the `json_parse` weave.op on the results to create a dictionary of the results. Then we wrap this dictionary using the `weave.ops.dict_` op to create a row. This entire operation is vectorized over the entire table to generate the results.\n",
    "\n",
    "## Panels\n",
    "The panels in the board are what gets displayed on the board. Here's a description of what's going on in each panel recognized by its `id`.\n",
    "\n",
    "1. `Description`: A markdown panel that displays our `info_message` variable.\n",
    "2. `Eval Prompt`: A string editable panel that displays our evaluation prompt. This allows users to edit the prompts as per their needs directly on the board.\n",
    "3. `Mean Coherence`: A number panel displaying the average coherence score of the evaluation. We use the `.avg()` weave.op over the table to achieve this and the next few panels.\n",
    "4. `Mean Correctness`: A number panel displaying the average correctness score of the evaluation over the entire dataset.\n",
    "5. `Mean Conciseness`: A number panel displaying the average conciseness score of the evaluation over the entire dataset.\n",
    "6. `Mean Relevance`: A number panel displaying the average relevance score of the evaluation over the entire dataset.\n",
    "7. `Coherence Distribution`: A histogram panel displaying the distribution of the coherence scores over the entire dataset.\n",
    "8. `Correctness Distribution`: A histogram panel displaying the distribution of the correctness scores over the entire dataset.\n",
    "9. `Conciseness Distribution`: A histogram panel displaying the distribution of the conciseness scores over the entire dataset.\n",
    "10. `Relevance Distribution`: A histogram panel displaying the distribution of the relevance scores over the entire dataset.\n",
    "11. `eval_table`: A weave Table panel showing the eval results. This is mostly renaming the columns in the `eval_results` for easier readability\n",
    "12. `Selected Result`: A panel that displays the active row selected in the above table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c4062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "board = weave.panels.Board(\n",
    "    vars={\n",
    "        \"info_message\": INFO_MESSAGE,\n",
    "        \"project\": weave.ops.project(\"parambharat\", \"weave\"),\n",
    "        \"qa_data\": lambda project : (project\n",
    "            .artifact(\"eval_data\")\n",
    "            .versions()[0]\n",
    "            .file(\"eval_dataset.table.json\")\n",
    "            .table()\n",
    "            .rows()\n",
    "        ),\n",
    "        \"eval_prompt\": EVALUATION_PROMPT,\n",
    "        \"model\": langchain.chat_openai('gpt-3.5-turbo', 0.7),\n",
    "        \"eval_results\": lambda eval_prompt, qa_data, model: qa_data.map(\n",
    "            lambda row, index: weave.ops.dict_(\n",
    "                question=row[\"question\"],\n",
    "                reference=row[\"reference\"],\n",
    "                answer=row[\"answer\"],\n",
    "                result=model.predict(\n",
    "                    eval_prompt \n",
    "                    .replace('{query}', row['question'])\n",
    "                    .replace('{reference}', row[\"reference\"])\n",
    "                    .replace('{answer}', row['answer'])\n",
    "                ).json_parse())).map(\n",
    "            lambda row, index: weave.ops.dict_(\n",
    "                question=row[\"question\"],\n",
    "                reference=row[\"reference\"],\n",
    "                answer=row[\"answer\"],\n",
    "                coherence_score=row['result.coherence.score'],\n",
    "                correctness_score=row['result.correctness.score'],\n",
    "                conciseness_score=row['result.conciseness.score'],\n",
    "                relevance_score=row['result.relevance.score'],\n",
    "                coherence_reason=row['result.coherence.reason'],\n",
    "                correctness_reason=row['result.correctness.reason'],\n",
    "                conciseness_reason=row['result.conciseness.reason'],\n",
    "                relevance_reason=row['result.relevance.reason']\n",
    "            )),\n",
    "    },\n",
    "    panels=[\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda info_message: weave.panels.PanelMarkdown(info_message),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=0, h=10, w=20),\n",
    "            id=\"Description\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_prompt: weave.panels.StringEditor(eval_prompt),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=0, h=10, w=20),\n",
    "            id=\"Eval Prompt\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: eval_results.map(lambda row, index: row[\"coherence_score\"]).avg(),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=10, h=5, w=5),\n",
    "            id=\"Mean Coherence\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: eval_results.map(lambda row, index: row[\"correctness_score\"]).avg(),\n",
    "            layout=weave.panels.BoardPanelLayout(x=5, y=10, h=5, w=5),\n",
    "            id=\"Mean Correctness\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: eval_results.map(lambda row, index: row[\"conciseness_score\"]).avg(),\n",
    "            layout=weave.panels.BoardPanelLayout(x=10, y=10, h=5, w=5),\n",
    "            id=\"Mean Conciseness\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: eval_results.map(lambda row, index: row[\"relevance_score\"]).avg(),\n",
    "            layout=weave.panels.BoardPanelLayout(x=15, y=10, h=5, w=5),\n",
    "            id=\"Mean Relevance\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: weave.panels.Histogram(eval_results[\"coherence_score\"]),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=15, h=5, w=5),\n",
    "            id=\"Coherence Distribution\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: weave.panels.Histogram(eval_results[\"correctness_score\"]),\n",
    "            layout=weave.panels.BoardPanelLayout(x=5, y=15, h=5, w=5),\n",
    "            id=\"Correctness Distribution\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: weave.panels.Histogram(eval_results[\"conciseness_score\"]),\n",
    "            layout=weave.panels.BoardPanelLayout(x=10, y=15, h=5, w=5),\n",
    "            id=\"Conciseness Distribution\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: weave.panels.Histogram(eval_results[\"relevance_score\"]), \n",
    "            layout=weave.panels.BoardPanelLayout(x=15, y=15, h=5, w=5),\n",
    "            id=\"Relevance Distribution\"),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_results: weave.panels.Table(\n",
    "                eval_results,\n",
    "                columns=[\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"question\"],\n",
    "                        name=\"question\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"reference\"],\n",
    "                        name=\"reference\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"answer\"],\n",
    "                        name=\"answer\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"correctness_score\"],\n",
    "                        name=\"correctness_score\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"coherence_score\"],\n",
    "                        name=\"coherence_score\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"conciseness_score\"],\n",
    "                        name=\"conciseness_score\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"relevance_score\"],\n",
    "                        name=\"relevance_score\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"correctness_reason\"],\n",
    "                        name=\"correctness_reason\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"coherence_reason\"],\n",
    "                        name=\"coherence_reason\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"conciseness_reason\"],\n",
    "                        name=\"conciseness_reason\",\n",
    "                    ),\n",
    "                    weave.panels.TableColumn(\n",
    "                        lambda row: row[\"relevance_reason\"],\n",
    "                        name=\"relevance_reason\",\n",
    "                    )\n",
    "                ]),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=20, h=10, w=20),\n",
    "            id=\"eval_table\",),\n",
    "        weave.panels.BoardPanel(\n",
    "            lambda eval_table: eval_table.active_row(),\n",
    "            layout=weave.panels.BoardPanelLayout(x=0, y=30, h=10, w=20),\n",
    "            id=\"Selected Result\",)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743b6c4-3cba-448d-9143-cc87874c63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the board\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44121843-e925-4776-bd9b-f42c683fa424",
   "metadata": {},
   "source": [
    "# Publishing\n",
    "Once the board is displayed in the above cell you can view an publish the weave board by clicking the publish button. Ensure to select the `project` and `entity` that are viewable by your team if you intend to share the board with your teammates. [Here's](https://weave.wandb.ai/?exp=get%28%0A++++%22wandb-artifact%3A%2F%2F%2Fparambharat%2Fweave%2FLLMAutoEvalDashBoard%3A1e78d5e897850fa3202a%2Fobj%22%29) an example of the board published,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea625c-f520-43d2-8159-d57868646805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
